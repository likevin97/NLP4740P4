{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import json\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import jaccard_similarity_score\n",
    "from scipy import spatial\n",
    "import matplotlib.pyplot as plt \n",
    "plt.rc(\"font\", size=14)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import pickle\n",
    "import string\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import time\n",
    "\n",
    "\n",
    "# Temp\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createCorpus(json_data, stopwords):\n",
    "    j = json_data\n",
    "    corpus = []\n",
    "    \n",
    "    for data in tqdm_notebook(j['data']):\n",
    "        for paragraph in data[\"paragraphs\"]:\n",
    "            context = paragraph[\"context\"]\n",
    "            corpus.append(context)\n",
    "            for q in paragraph[\"qas\"]:\n",
    "                question = q[\"question\"]\n",
    "                corpus.append(question)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPOS(corpus, progress = True):\n",
    "    pos_tag = []\n",
    "    c = tqdm_notebook(corpus) if progress else corpus\n",
    "    for sentence in c:\n",
    "        text = nltk.word_tokenize(sentence)\n",
    "        pos_tag = nltk.pos_tag(text)\n",
    "    return pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "563727d8950d4dfd803e4864bf0950b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8a4fb1c0c304e9fa52366ab4e65ae8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Something': 'v', 'is': 'v', 'weird': None}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def makePOSDict(corpus):\n",
    "    pos = getPOS(corpus)\n",
    "    pos_dict = {}\n",
    "    for (word, tag) in tqdm_notebook(pos):\n",
    "        wtag = tag[0].lower()\n",
    "        wtag = wtag if wtag in [\"a\",\"r\",\"n\",\"v\"] else None\n",
    "        pos_dict[word] = wtag\n",
    "        \n",
    "    return pos_dict\n",
    "\n",
    "makePOSDict([\"Something is weird\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ourLemmatize(dictonary, corpus):\n",
    "    lemma = WordNetLemmatizer()\n",
    "    corp = []\n",
    "    for sentence in tqdm_notebook(corpus):\n",
    "        s = nltk.word_tokenize(sentence)\n",
    "        sent = []\n",
    "        for word in s:\n",
    "            if dictonary[word] != None:\n",
    "                nw = lemma.lemmatize(word, dictonary[word])\n",
    "                sent.append(nw)\n",
    "            else:\n",
    "                sent.append(word)\n",
    "#         corp.append(\" \".join(sent))\n",
    "        corp.append(sent)\n",
    "    return corp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make and load pickles\n",
    "def makePickle(filename, data_file):\n",
    "    pickle_out = open(\"pickles/\" + filename + \".pickle\",\"wb\")\n",
    "    pickle.dump(data_file, pickle_out)\n",
    "    pickle_out.close()\n",
    "    \n",
    "def loadPickle(filename):\n",
    "    pickle_in = open(\"pickles/\" + filename + \".pickle\",\"rb\")\n",
    "    return pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeFeatures(features, pred = None):\n",
    "    # dict to array\n",
    "    y = []\n",
    "    xs = []\n",
    "    for k,v in features[0].items():\n",
    "        x = [feature[k] for feature in features]\n",
    "        \n",
    "        xs.append(x)\n",
    "        if pred != None:\n",
    "            y.append(pred[k])\n",
    "        else:\n",
    "            y.append(k)\n",
    "        \n",
    "    Y = np.array(y)\n",
    "    X = np.array(xs)\n",
    "    \n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHotEncode(columnId, feature_data):\n",
    "    # Select categorical data\n",
    "    df = pd.DataFrame(data=feature_data)\n",
    "    newdf = df.iloc[:,columnId:columnId+1]\n",
    "    \n",
    "    le = preprocessing.LabelEncoder()\n",
    "    enc = preprocessing.OneHotEncoder()\n",
    "    \n",
    "    X_1 = df.drop([columnId],1)\n",
    "\n",
    "    X_2 = newdf.apply(le.fit_transform)\n",
    "    enc.fit(X_2)\n",
    "    onehotlabels = enc.transform(X_2).toarray()\n",
    "    X_2 =  pd.DataFrame(data=onehotlabels)\n",
    "    \n",
    "    feature_data = pd.concat([X_1, X_2], axis=1, sort=False)\n",
    "    return feature_data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveToFile(preds, keys, filename):\n",
    "    predictions = {}\n",
    "    for i in range(len(keys)):\n",
    "        key = keys[i]\n",
    "        predictions[key] = preds[i]\n",
    "    \n",
    "    with open(filename + '.csv', 'w') as f:\n",
    "        f.write(\"Id,Category\\n\")\n",
    "        for k,v in predictions.items():\n",
    "            f.write(\"%s,%s\\n\"%(k,v))\n",
    "\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('training.json') as f:\n",
    "    json_data = json.load(f)\n",
    "with open('development.json') as f:\n",
    "    json_data_validate = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What was the first album Beyoncé released as a solo artist?'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    corpus = loadPickle('corpus')\n",
    "except:\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.union(set(string.punctuation))\n",
    "    corpus = createCorpus(json_data, stop_words)\n",
    "    makePickle('corpus', corpus)\n",
    "\n",
    "corpus[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'When did Beyoncé release Dangerously in Love?'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    corpus_val = loadPickle('corpus_val')\n",
    "except:\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.union(set(string.punctuation))\n",
    "    corpus_val = createCorpus(json_data_validate, stop_words)\n",
    "    makePickle('corpus_val', corpus_val)\n",
    "\n",
    "corpus_val[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Dict and Lemmatized corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Beyoncé', 'n')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    pos_dict = loadPickle('pos_dic')\n",
    "except:\n",
    "    pos_dict = makePOSDict(corpus)\n",
    "    makePickle('pos_dic', pos_dict)\n",
    "    \n",
    "next(iter(pos_dict.keys())), next(iter(pos_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What', 'be', 'the', 'first']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    corpus_lemmatize = loadPickle('corpus_lemmatize')\n",
    "except:\n",
    "    corpus_lemmatize = ourLemmatize(pos_dict, corpus)\n",
    "    makePickle('corpus_lemmatize', corpus_lemmatize)\n",
    "\n",
    "corpus_lemmatize[1][:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Beyoncé', 'n')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    pos_dic_val = loadPickle('pos_dic_val')\n",
    "except:\n",
    "    pos_dic_val = makePOSDict(corpus_val)\n",
    "    makePickle('pos_dic_val', pos_dic_val)\n",
    "    \n",
    "next(iter(pos_dic_val.keys())), next(iter(pos_dic_val.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['When', 'do', 'Beyoncé', 'release']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    corpus_lemmatize_val = loadPickle('corpus_lemmatize_val')\n",
    "except:\n",
    "    corpus_lemmatize_val = ourLemmatize(pos_dic_val, corpus_val)\n",
    "    makePickle('corpus_lemmatize_val', corpus_lemmatize_val)\n",
    "    \n",
    "corpus_lemmatize_val[1][:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPredictions(json_data):\n",
    "    pred = {}\n",
    "    for obj in tqdm_notebook(json_data[\"data\"]):\n",
    "        for para in obj[\"paragraphs\"]:\n",
    "            for q in para[\"qas\"]:\n",
    "                question_id = q[\"id\"]\n",
    "                pred[question_id] = 0 if q[\"is_impossible\"] else 1\n",
    "                \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7282e230ce364f75a0a8f33028685026",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=442), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0c764ebe2ec4a099ac99eb8d969fdd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=442), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pred =  getPredictions(json_data)\n",
    "pred_validate = getPredictions(json_data_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jaccard Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6842105263157895"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Jaccard Similarity function\n",
    "def DistJaccard(str1, str2):\n",
    "    str1 = set(str1)\n",
    "    str2 = set(str2)\n",
    "    jd = nltk.jaccard_distance(str1, str2)\n",
    "#     manual = float(len(str1 & str2)) / len(str1 | str2)\n",
    "    return jd\n",
    "\n",
    "DistJaccard(\"something stuped okay lol\",\"Okay cool lol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getJaccard(json_data, corpus):\n",
    "    jac_similarity = {}\n",
    "    counter = 0\n",
    "    for obj in tqdm_notebook(json_data[\"data\"]):\n",
    "        for para in obj[\"paragraphs\"]:\n",
    "            context = corpus[counter]\n",
    "\n",
    "            counter += 1\n",
    "            for q in para[\"qas\"]:\n",
    "                question_id = q[\"id\"]\n",
    "                question = corpus[counter]\n",
    "                \n",
    "                sim = np.array([DistJaccard(sent,question) for sent in context.split(\".\")])\n",
    "                jac_similarity[question_id] = np.mean(sim)\n",
    "                \n",
    "                counter += 1\n",
    "                \n",
    "    return jac_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2276edf31f204f0ab5621341cc9dc68c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=442), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "jac_similarity = getJaccard(json_data, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ed01497681a43f2a5cc52bf7d181741",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=442), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "jac_similarity_val = getJaccard(json_data_validate, corpus_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def documentVector(doc, WEModel):\n",
    "    # remove out-of-vocabulary words\n",
    "    doc = [word for word in doc if word in WEModel.vocab]\n",
    "    try:\n",
    "        return np.mean(WEModel[doc], axis=0)\n",
    "    except:\n",
    "        return np.zeros(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCosine(WEModel, json_data, corp):\n",
    "    cos_similarity = {}\n",
    "    counter = 0\n",
    "    for obj in tqdm_notebook(json_data[\"data\"]):\n",
    "        for para in obj[\"paragraphs\"]:\n",
    "\n",
    "            context = corp[counter]\n",
    "            context_vector = documentVector(context, WEModel)\n",
    "\n",
    "            counter += 1\n",
    "            for q in para[\"qas\"]:\n",
    "                question_id = q[\"id\"]\n",
    "                question_vector = documentVector(corp[counter], WEModel)\n",
    "                \n",
    "                sim = np.array([cosine_similarity(np.array([documentVector(s, WEModel), question_vector]))[0][1] for s in context.split(\".\")])\n",
    "                # cos_similarity[question_id] = cosine_similarity(np.array([context_vector, question_vector]))[0][1]\n",
    "                cos_similarity[question_id] = np.mean(sim)\n",
    "                \n",
    "                counter += 1\n",
    "                \n",
    "    return cos_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "word_2vec_cached = KeyedVectors.load_word2vec_format(\"WE/GoogleNews-vectors-negative300.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a3633fdc1804907875ae07e5a680a63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=442), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cos_similarity = getCosine(word_2vec_cached, json_data, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_similarity_val = getCosine(word_2vec_cached, json_data_validate, corpus_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_words = [\"who\", \"what\", \"when\", \"where\", \"why\", \"how\", \"which\", \"whose\", \"whom\", \"is\", \"was\", \"are\", \"does\", \"did\", \"were\", \"can\", \"do\", \"has\", \"had\", \"name\"]\n",
    "len(question_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'when'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getQuestionWord(question, question_words):\n",
    "    question_array = [question]\n",
    "    question_pos = getPOS(question_array, False)\n",
    "    q_word = \"<unk>\"\n",
    "\n",
    "    for (w,t) in question_pos:\n",
    "        word = w.lower()\n",
    "        if word in question_words:\n",
    "            q_word = word\n",
    "            break\n",
    "    \n",
    "    return q_word\n",
    "\n",
    "getQuestionWord(\"When did Beyoncé release Dangerously in Love?\", question_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getQuestionWords(json_data, corp, question_words):\n",
    "    words = {}\n",
    "    counter = 0\n",
    "    for obj in tqdm_notebook(json_data[\"data\"]):\n",
    "        for para in obj[\"paragraphs\"]:\n",
    "\n",
    "            counter += 1\n",
    "            for q in para[\"qas\"]:\n",
    "                question_id = q[\"id\"]\n",
    "                words[question_id] = getQuestionWord(corp[counter], question_words)\n",
    "                \n",
    "                counter += 1\n",
    "                \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questionWords = getQuestionWords(json_data, corpus, question_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questionWords_val = getQuestionWords(json_data_validate, corpus_val, question_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "whoTags = [\"B-PERSON\", \"I-PERSON\"]\n",
    "whatTags = [\"B-GPE\", \"I-GPE\", \"B-LOCATION\", \"I_LOCATION\"]\n",
    "whenTags = [\"B-DATE\", \"I-DATE\", \"B-TIME\", \"I-TIME\"]\n",
    "otherTags = [\"B-PERSON\", \"I-PERSON\", \"B-GPE\", \"I-GPE\", \"B-LOCATION\", \"I-LOCATION\", \"B-DATE\", \"I-DATE\", \"B-TIME\", \"I-TIME\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maps question word to NER\n",
    "qwordTagMap = {}\n",
    "for question_word in question_words:\n",
    "    qwordTagMap[question_word] = otherTags\n",
    "    qwordTagMap[\"who\"] = whoTags\n",
    "    qwordTagMap[\"what\"] = whatTags\n",
    "    qwordTagMap[\"when\"] = whenTags\n",
    "    qwordTagMap[\"<unk>\"] = otherTags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getNERCount(question_words, q_word, question, context, qwordTagMap):\n",
    "    '''\n",
    "    for the question word, does the corresponding NER tag exist in the context.\n",
    "    For every NER tag exists, see if that word is in the question\n",
    "    if yes dont count, else count\n",
    "    '''\n",
    "    qwordTags = qwordTagMap[q_word]\n",
    "    num_of_tags_in_context = 0\n",
    "    \n",
    "    context_words = nltk.word_tokenize(context)\n",
    "    context_chunks = nltk.chunk.util.tree2conlltags(nltk.ne_chunk(nltk.pos_tag(context_words)))\n",
    "\n",
    "    if (len(context_chunks) > 0):\n",
    "        for context_chunk in context_chunks:\n",
    "            if (context_chunk[2] in qwordTags and context_chunk[0] not in question):\n",
    "                num_of_tags_in_context += 1\n",
    "\n",
    "    return num_of_tags_in_context\n",
    "\n",
    "getNERCount(question_words,\"when\", \"When was the first album Beyoncé released as a solo artist?\", \"Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress\", qwordTagMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNERCounts(json_data, corp, questionKey, question_words, qwordTagMap):\n",
    "    counts = {}\n",
    "    counter = 0\n",
    "    for obj in tqdm(json_data[\"data\"]):\n",
    "        for para in obj[\"paragraphs\"]:\n",
    "            \n",
    "            context = corp[counter]\n",
    "            counter += 1\n",
    "            for q in para[\"qas\"]:\n",
    "                question_id = q[\"id\"]\n",
    "                counts[question_id] = getNERCount(question_words, questionKey[question_id], corp[counter], context, qwordTagMap)\n",
    "                \n",
    "                counter += 1\n",
    "                \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    nerCounts = loadPickle('nerCounts')\n",
    "except:\n",
    "    nerCounts = getNERCounts(json_data, corpus, questionWords, question_words, qwordTagMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    nerCountsVal = loadPickle('nerCounts_val')\n",
    "except:\n",
    "    nerCountsVal = getNERCounts(json_data_validate, corpus_val, questionWords_val, question_words, qwordTagMap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verb Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['do', 'grow'], ['city', 'state', 'Beyonce'])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getQuestionVerbsNouns(question_array, pos_dict):\n",
    "    question_pos = [(w, pos_dict[W]) for (w, W) in question_array]\n",
    "    q_verbs = []\n",
    "    q_nouns = []\n",
    "    \n",
    "    for (w,t) in question_pos:\n",
    "        if t == \"v\":\n",
    "            q_verbs.append(w)\n",
    "        elif t == \"n\":\n",
    "            q_nouns.append(w)\n",
    "    \n",
    "    return q_verbs, q_nouns\n",
    "\n",
    "arr = 'In what city and state do Beyonce grow up ?'.split(\" \")\n",
    "arr = zip(arr,arr)\n",
    "getQuestionVerbsNouns(arr, pos_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getVerbNounCount(question_array, context_set, pos_dict):\n",
    "    q_verbs, q_nouns = getQuestionVerbsNouns(question_array, pos_dict)\n",
    "    return len(context_set & set(q_verbs)), len(context_set & set(q_nouns))\n",
    "\n",
    "getVerbNounCount(zip(corpus_lemmatize[1],corpus_lemmatize[1]), set(corpus_lemmatize[0]), pos_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVerbNounCounts(json_data, corp, corp_lemmatized, pos_dict):\n",
    "    verbCounts = {}\n",
    "    nounCounts = {}\n",
    "    counter = 0\n",
    "    for obj in tqdm_notebook(json_data[\"data\"]):\n",
    "        for para in obj[\"paragraphs\"]:\n",
    "            \n",
    "            context_array = corp_lemmatized[counter]\n",
    "            context_set = set(context_array)\n",
    "            counter += 1\n",
    "            for q in para[\"qas\"]:\n",
    "                question_id = q[\"id\"]\n",
    "                question_lem_array = corp_lemmatized[counter]\n",
    "                question_array = nltk.word_tokenize(corp[counter])\n",
    "                arr = zip(question_lem_array, question_array)\n",
    "                verbCounts[question_id], nounCounts[question_id] = getVerbNounCount(arr, context_set, pos_dict)\n",
    "                \n",
    "                counter += 1\n",
    "                \n",
    "    return verbCounts, nounCounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbCounts,nounCounts  = getVerbNounCounts(json_data, corpus, corpus_lemmatize, pos_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbCounts_val,nounCounts_val  = getVerbNounCounts(json_data_validate, corpus_val, corpus_lemmatize_val, pos_dic_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['When', 'was', 'first', 'album', 'Beyoncé', 'released', 'solo', 'artist']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getSimpleQuestion(question):\n",
    "    text = nltk.word_tokenize(question)\n",
    "    words = nltk.pos_tag(text)\n",
    "    less_words = [wt for (wt, tag) in words if tag not in [\"CC\",\"DT\",\"EX\",\"IN\",\"LS\",\"POS\",\"TO\",\".\",\"\\\\\",\",\",\":\",\"(\",\")\"]]\n",
    "    return less_words\n",
    "\n",
    "getSimpleQuestion(\"When was the first album Beyoncé released as a solo artist?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'When': 2, 'was': 1})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def countWordsInParagraph(context):\n",
    "    wordList = nltk.word_tokenize(context)\n",
    "    counts = Counter(wordList)\n",
    "    return counts\n",
    "\n",
    "countWordsInParagraph(\"When When was\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSimilarWordCount(json_data, corp):\n",
    "    counter = 0\n",
    "    predictions = {}\n",
    "    for obj in tqdm_notebook(json_data[\"data\"]):\n",
    "        for para in obj[\"paragraphs\"]:\n",
    "            \n",
    "            context = corp[counter]\n",
    "            dic = countWordsInParagraph(context)\n",
    "            counter += 1\n",
    "            for q in para[\"qas\"]:\n",
    "                question_id = q[\"id\"]\n",
    "                question = corpus[counter]\n",
    "                new_question = getSimpleQuestion(question)\n",
    "                total = 0\n",
    "                for word in new_question:\n",
    "                    if word in dic:\n",
    "                        total+= dic[word]\n",
    "                        \n",
    "                predictions[question_id] = total\n",
    "                \n",
    "                counter += 1\n",
    "                \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarWords = getSimilarWordCount(json_data, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarWords_val = getSimilarWordCount(json_data_validate, corpus_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = similarWords\n",
    "prediction = pred\n",
    "\n",
    "# dict to array\n",
    "y = []\n",
    "x = []\n",
    "for k,v in feature.items():\n",
    "    x.append(v)\n",
    "    y.append(prediction[k])\n",
    "    \n",
    "xy = np.array([x,y])\n",
    "data = pd.DataFrame(data=np.transpose(xy))\n",
    "data.head()\n",
    "data.plot(kind='scatter', x=1, y=0)\n",
    "plt.ylabel('Feature score')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    features = loadPickle(\"features\")\n",
    "except:\n",
    "    features = [jac_similarity, cos_similarity, nerCounts, nounCounts, verbCounts, similarWords, questionWords]\n",
    "    \n",
    "X_train,y_train = makeFeatures(features, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    features = loadPickle(\"features_val\")\n",
    "except:\n",
    "    features = [jac_similarity_val, cos_similarity_val, nerCountsVal, nounCounts_val, verbCounts_val, similarWords_val, questionWords_val]\n",
    "\n",
    "X_test,y_test = makeFeatures(features, pred_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train = oneHotEncode(6, X_train)\n",
    "X_test = oneHotEncode(6, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(69596, 17400, 69596, 17400)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train),len(X_test),len(y_train),len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all parameters not specified are set to their defaults\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train) \n",
    "X_test = scaler.transform(X_test)  \n",
    "logisticRegr = LogisticRegression()\n",
    "logisticRegr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=logisticRegr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 257.44, 'Predicted label')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_names=[0,1] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "# create heatmap\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5328735632183909\n",
      "Precision: 0.5719677906391545\n",
      "Recall: 0.26126436781609197\n",
      "F-Score: 0.35868707590342436\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))\n",
    "print(\"F-Score:\",metrics.f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.json') as f:\n",
    "    test_json_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    corpus_test = loadPickle('corpus_test')\n",
    "except:\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.union(set(string.punctuation))\n",
    "    corpus_test = createCorpus(json_data, stop_words)\n",
    "    makePickle('corpus_test', corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Beyoncé', 'n')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    pos_dict_test = loadPickle('pos_dic_test')\n",
    "except:\n",
    "    pos_dict_test = makePOSDict(corpus_test)\n",
    "    makePickle('pos_dic_test', pos_dict_test)\n",
    "    \n",
    "next(iter(pos_dict_test.keys())), next(iter(pos_dict_test.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    corpus_lemmatize_test = loadPickle('corpus_lemmatize_test')\n",
    "except:\n",
    "    corpus_lemmatize_test = ourLemmatize(pos_dict_test, corpus_test)\n",
    "    makePickle('corpus_lemmatize_test', corpus_lemmatize_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fde879acca3d410aa95d0eb02dc1ea93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=35), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8db41b114594e5db16c31ec704a28a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=35), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73c555b4a0094105858afccef6b650be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=35), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "891d31e2f14f42bd996421abcb4a51d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=35), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89e41cb4629046b090d9e24313896ac2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=35), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-cc53eb288868>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_questionWords\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mgetQuestionWords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_json_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_verbCounts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_nounCounts\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mgetVerbNounCounts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_json_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_lemmatize_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_dict_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtest_similarWords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetSimilarWordCount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_json_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-43-3df647087afc>\u001b[0m in \u001b[0;36mgetSimilarWordCount\u001b[0;34m(json_data, corp)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpara\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"qas\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0mquestion_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                 \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m                 \u001b[0mnew_question\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetSimpleQuestion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'corpus' is not defined"
     ]
    }
   ],
   "source": [
    "test_jac_similarity = getJaccard(test_json_data, corpus_test)\n",
    "test_cos_similarity = getCosine(word_2vec_cached, test_json_data, corpus_test)\n",
    "test_questionWords  = getQuestionWords(test_json_data, corpus_test, question_words)\n",
    "test_verbCounts, test_nounCounts  = getVerbNounCounts(test_json_data, corpus_test, corpus_lemmatize_test, pos_dict_test)\n",
    "test_similarWords = getSimilarWordCount(test_json_data, corpus_test)\n",
    "\n",
    "try:\n",
    "    test_nerCounts = loadPickle('test_nerCounts')\n",
    "except:\n",
    "    test_nerCounts = getNERCounts(test_json_data, corpus_test, test_questionWords, question_words, qwordTagMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "features = [test_jac_similarity, test_cos_similarity, test_verbCounts, test_nounCounts, test_nerCounts, test_similarWords, test_questionWords]\n",
    "X_out,keys = makeFeatures(features)\n",
    "X_out = oneHotEncode(6, X_out)\n",
    "y_pred_test = logisticRegr.predict(X_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveToFile(y_pred_test, keys, \"out_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
